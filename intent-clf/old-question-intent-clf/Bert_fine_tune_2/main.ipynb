{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load in the data file\n",
    "with open('../question-data.json') as data_file:\n",
    "    data = json.loads(data_file.read())\n",
    "\n",
    "# Prepare the data\n",
    "td_text = []\n",
    "td_labels = []\n",
    "str_to_num = {'injury': 0, 'trade': 1}\n",
    "for index in data:\n",
    "    td_text += [data[index][x]for x in range(len(data[index]))]\n",
    "    td_labels += [str_to_num[index] for x in range(len(data[index]))]\n",
    "#final = {\"text\": td_text, \"label\": td_labels}\n",
    "final = [{\"text\": td_text[i], \"label\": td_labels[i], \"idx\": i} for i in range(len(td_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/danielstefanescu/.cache/huggingface/hub/models--facebook--bart-large-mnli/snapshots/c626438eeca63a93bd6024b0a0fbf8b3c0c30d7b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/danielstefanescu/.cache/huggingface/hub/models--facebook--bart-large-mnli/snapshots/c626438eeca63a93bd6024b0a0fbf8b3c0c30d7b/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-mnli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"entailment\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 2,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, TFBertModel, BertConfig\n\u001b[0;32m----> 3\u001b[0m bert_tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/bart-large-mnli\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m bert_model \u001b[39m=\u001b[39m TFBertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mfacebook/bart-large-mnli\u001b[39m\u001b[39m'\u001b[39m,num_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1775\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1772\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1775\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1776\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1777\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1778\u001b[0m     init_configuration,\n\u001b[1;32m   1779\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1780\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1781\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1782\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1783\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1784\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1785\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1930\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1929\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1930\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1931\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1932\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1933\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1934\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1935\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py:213\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    185\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    186\u001b[0m     vocab_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    198\u001b[0m ):\n\u001b[1;32m    199\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    200\u001b[0m         do_lower_case\u001b[39m=\u001b[39mdo_lower_case,\n\u001b[1;32m    201\u001b[0m         do_basic_tokenize\u001b[39m=\u001b[39mdo_basic_tokenize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misfile(vocab_file):\n\u001b[1;32m    214\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a vocabulary file at path \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mvocab_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. To load the vocabulary from a Google pretrained\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         )\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m=\u001b[39m load_vocab(vocab_file)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[1;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('facebook/bart-large-mnli',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielstefanescu/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for sent in td_text:\n",
    "    bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =64,pad_to_max_length = True,return_attention_mask = True)\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "input_ids=np.asarray(input_ids)\n",
    "attention_masks=np.array(attention_masks)\n",
    "labels=np.array(td_labels)\n",
    "\n",
    "train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
    "\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print (train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 15:51:33.868814: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.6073 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 15:51:47.320448: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 23s 23s/step - loss: 0.6073 - accuracy: 0.5000 - val_loss: 0.5493 - val_accuracy: 1.0000\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5572 - accuracy: 1.0000 - val_loss: 0.5444 - val_accuracy: 0.8750\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5904 - accuracy: 0.8000 - val_loss: 0.4917 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 1s 873ms/step - loss: 0.5549 - accuracy: 0.7000 - val_loss: 0.4638 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history=bert_model.fit([train_inp,train_mask],train_label,batch_size=32,epochs=4,validation_data=([val_inp,val_mask],val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 522ms/step\n",
      "    label     score\n",
      "0  injury  0.556148\n",
      "1  injury  0.541180\n",
      "2  injury  0.555820\n"
     ]
    }
   ],
   "source": [
    "def extract_predictions(text):\n",
    "  a_preds = []\n",
    "  a_scores = []\n",
    "  data_encodings = bert_tokenizer(text, truncation=True, padding=True)\n",
    "  ds = tf.data.Dataset.from_tensor_slices(dict(data_encodings)).batch(64)\n",
    "  preds = bert_model.predict(ds)[\"logits\"]\n",
    "  classes = np.argmax(preds, axis=1).tolist()\n",
    "  probs = tf.nn.softmax(preds) # get probabilities from logits\n",
    "  scores = np.amax(probs, axis=1).tolist()\n",
    "\n",
    "  a_preds.extend(classes)\n",
    "  a_scores.extend(scores)\n",
    "  # return predictions and scores\n",
    "  ans = []\n",
    "  num_to_str = {0: 'injury', 1: 'trade'}\n",
    "  for i in range(len(a_preds)):  # map labels to label titles\n",
    "    ans.append({\n",
    "        \"label\": num_to_str[a_preds[i]],\n",
    "        \"score\": a_scores[i]\n",
    "    })\n",
    "  ansdf = pd.DataFrame(ans)\n",
    "  return ansdf\n",
    "\n",
    "text = [\"when will [player] be back?\", \"is [player] hurt?\"]\n",
    "result = extract_predictions(text)\n",
    "print (result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb7855f218f64c911181e5eed165e911150e68e42eabb3ab704de57f219c0a28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
