{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load in the data file\n",
    "with open('../question-data.json') as data_file:\n",
    "    data = json.loads(data_file.read())\n",
    "\n",
    "# Prepare the data\n",
    "td_text = []\n",
    "td_labels = []\n",
    "for index in data:\n",
    "    td_text += [data[index][x]for x in range(len(data[index]))]\n",
    "    td_labels += [index for x in range(len(data[index]))]\n",
    "#final = {\"text\": td_text, \"label\": td_labels}\n",
    "str_to_num = {'injury': 0, 'trade': 1}\n",
    "final = [{\"text\": td_text[i], \"label\": str_to_num[td_labels[i]], \"idx\": i} for i in range(len(td_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for sent in td_text:\n",
    "    bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =64,pad_to_max_length = True,return_attention_mask = True)\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "input_ids=np.asarray(input_ids)\n",
    "attention_masks=np.array(attention_masks)\n",
    "labels=np.array(td_labels)\n",
    "\n",
    "train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a json file\n",
    "import json\n",
    "json_object = json.dumps(final, indent=4)\n",
    "with open(\"final_data.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-743fe2d2f16f0deb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/danielstefanescu/.cache/huggingface/datasets/json/default-743fe2d2f16f0deb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3697af8fd4f4170bcfa067fc8f6089e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce7e2941b8245528613d315eefc27a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f48c07abd8449cb2e2a8060fc83169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/danielstefanescu/.cache/huggingface/datasets/json/default-743fe2d2f16f0deb/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd00166030ef45beb9feaca22174bc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='final_data.json') #, split=['train[:80%]', 'train[20%:]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9332a285e4664f7ca45199a11ebf95bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Label column labels not found in dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf_train_dataset \u001b[39m=\u001b[39m tokenized_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto_tf_dataset(\n\u001b[1;32m      2\u001b[0m     columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtoken_type_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      3\u001b[0m     label_cols\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      4\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      5\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/datasets/arrow_dataset.py:447\u001b[0m, in \u001b[0;36mTensorflowDatasetMixin.to_tf_dataset\u001b[0;34m(self, batch_size, columns, shuffle, collate_fn, drop_remainder, collate_fn_args, label_cols, prefetch)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m label_cols:\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m output_signature:\n\u001b[0;32m--> 447\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLabel column \u001b[39m\u001b[39m{\u001b[39;00mcol\u001b[39m}\u001b[39;00m\u001b[39m not found in dataset!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnp_get_batch\u001b[39m(indices):\n\u001b[1;32m    450\u001b[0m     \u001b[39m# Optimization - if we're loading a sequential batch, do it with slicing instead of a list of indices\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mall(np\u001b[39m.\u001b[39mdiff(indices) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Label column labels not found in dataset!"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# tf_validation_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "#     columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "#     label_cols=[\"labels\"],\n",
    "#     shuffle=False,\n",
    "#     collate_fn=data_collator,\n",
    "#     batch_size=8,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-10-20 14:46:00.128626: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 107s 24s/step - loss: 0.7141 - sparse_categorical_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6bf041580>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification,TFBertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Training the model\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2 )\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=5e-5,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=tf.metrics.SparseCategoricalAccuracy()\n",
    "              ) # can also use any keras loss fn\n",
    "model.fit(tf_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5298250913619995},\n",
       " {'label': 'LABEL_0', 'score': 0.5871033072471619},\n",
       " {'label': 'LABEL_0', 'score': 0.5464733839035034}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "text = [\"is [player] staying at [team]?\", \"is [player] injured?\", \"Is [player] staying at [team]?\"]\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 15:01:41.289275: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 19s 19s/step\n",
      "    label     score\n",
      "0  injury  0.529825\n",
      "1  injury  0.587103\n",
      "2  injury  0.546473\n"
     ]
    }
   ],
   "source": [
    "def extract_predictions(text):\n",
    "  a_preds = []\n",
    "  a_scores = []\n",
    "  data_encodings = tokenizer(text, truncation=True, padding=True)\n",
    "  ds = tf.data.Dataset.from_tensor_slices(dict(data_encodings)).batch(64)\n",
    "  preds = model.predict(ds)[\"logits\"]\n",
    "  classes = np.argmax(preds, axis=1).tolist()\n",
    "  probs = tf.nn.softmax(preds) # get probabilities from logits\n",
    "  scores = np.amax(probs, axis=1).tolist()\n",
    "\n",
    "  a_preds.extend(classes)\n",
    "  a_scores.extend(scores)\n",
    "  # return predictions and scores\n",
    "  ans = []\n",
    "  num_to_str = {0: 'injury', 1: 'trade'}\n",
    "  for i in range(len(a_preds)):  # map labels to label titles\n",
    "    ans.append({\n",
    "        \"label\": num_to_str[a_preds[i]],\n",
    "        \"score\": a_scores[i]\n",
    "    })\n",
    "  ansdf = pd.DataFrame(ans)\n",
    "  return ansdf\n",
    "\n",
    "text = [\"is [player] staying at [team]?\", \"is [player] injured?\", \"Is [player] staying at [team]?\"]\n",
    "result = extract_predictions(text)\n",
    "print (result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb7855f218f64c911181e5eed165e911150e68e42eabb3ab704de57f219c0a28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
